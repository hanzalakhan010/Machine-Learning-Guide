{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Supervised Learning Revision Guide**\n",
    "\n",
    "#### **1. Overview**\n",
    "- **Definition**: Supervised learning is a type of machine learning where the model is trained on labeled data to predict an output for unseen data.\n",
    "- **Key Features**: \n",
    "  - Input-output pairs.\n",
    "  - Goal: Minimize error/loss.\n",
    "- **Common Tasks**:\n",
    "  - **Regression**: Predict continuous values (e.g., house prices, stock prices).\n",
    "  - **Classification**: Predict categorical labels (e.g., spam or not spam, image classes).\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Key Terminology**\n",
    "- **Training Set**: Data used to train the model.\n",
    "- **Test Set**: Data used to evaluate model performance.\n",
    "- **Features**: Input variables (independent variables).\n",
    "- **Target**: Output variable (dependent variable).\n",
    "- **Overfitting**: When a model performs well on training data but poorly on unseen data.\n",
    "- **Underfitting**: When a model is too simple and performs poorly on both training and test data.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Common Algorithms**\n",
    "**Regression**:\n",
    "1. **Linear Regression**: Predicts a continuous target based on a linear relationship.\n",
    "   - Formula: \\( Y = \\beta_0 + \\beta_1X + \\epsilon \\)\n",
    "   - Metrics: Mean Squared Error (MSE), R².\n",
    "\n",
    "2. **Ridge & Lasso Regression**: Variants of linear regression with regularization to prevent overfitting.\n",
    "\n",
    "3. **Polynomial Regression**: Models non-linear relationships by adding polynomial features.\n",
    "\n",
    "**Classification**:\n",
    "1. **Logistic Regression**: Predicts binary outcomes (e.g., Yes/No).\n",
    "   - Output: Probability.\n",
    "   - Activation: Sigmoid function.\n",
    "\n",
    "2. **Support Vector Machine (SVM)**:\n",
    "   - Separates classes using a hyperplane.\n",
    "   - Works well with high-dimensional data.\n",
    "\n",
    "3. **K-Nearest Neighbors (KNN)**:\n",
    "   - Instance-based algorithm.\n",
    "   - Predicts based on the majority class of \\( k \\)-nearest neighbors.\n",
    "\n",
    "4. **Decision Trees**:\n",
    "   - Splits data based on feature thresholds.\n",
    "   - Prone to overfitting.\n",
    "\n",
    "5. **Random Forest**:\n",
    "   - Ensemble of decision trees.\n",
    "   - Reduces overfitting.\n",
    "\n",
    "6. **Gradient Boosting**:\n",
    "   - Combines weak learners sequentially to improve performance.\n",
    "   - Examples: XGBoost, LightGBM, CatBoost.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Model Evaluation**\n",
    "- **Metrics for Regression**:\n",
    "  - Mean Absolute Error (MAE)\n",
    "  - Mean Squared Error (MSE)\n",
    "  - Root Mean Squared Error (RMSE)\n",
    "  - R-squared (R²)\n",
    "\n",
    "- **Metrics for Classification**:\n",
    "  - Accuracy\n",
    "  - Precision\n",
    "  - Recall\n",
    "  - F1-Score\n",
    "  - ROC-AUC\n",
    "\n",
    "- **Cross-Validation**:\n",
    "  - K-Fold Cross-Validation.\n",
    "  - Stratified K-Fold for imbalanced datasets.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. Preprocessing Techniques**\n",
    "- **Feature Scaling**:\n",
    "  - Standardization (z-score normalization).\n",
    "  - Min-Max Scaling.\n",
    "\n",
    "- **Handling Missing Values**:\n",
    "  - Mean/Median/Mode Imputation.\n",
    "  - Dropping rows/columns.\n",
    "\n",
    "- **Encoding Categorical Variables**:\n",
    "  - One-Hot Encoding.\n",
    "  - Label Encoding.\n",
    "\n",
    "---\n",
    "\n",
    "#### **6. Hyperparameter Tuning**\n",
    "- **Grid Search**: Exhaustive search over a specified parameter grid.\n",
    "- **Random Search**: Randomly samples parameters.\n",
    "- **Bayesian Optimization**: More efficient parameter search.\n",
    "\n",
    "---\n",
    "\n",
    "#### **7. Challenges**\n",
    "- **Imbalanced Datasets**:\n",
    "  - Use techniques like SMOTE, undersampling, or oversampling.\n",
    "  - Adjust class weights in models.\n",
    "\n",
    "- **Feature Selection**:\n",
    "  - Use methods like Recursive Feature Elimination (RFE), Lasso, or feature importance from tree-based models.\n",
    "\n",
    "---\n",
    "\n",
    "#### **8. Tools and Libraries**\n",
    "- **Python Libraries**:\n",
    "  - `scikit-learn`: Core library for ML models.\n",
    "  - `pandas`: Data manipulation.\n",
    "  - `matplotlib`/`seaborn`: Visualization.\n",
    "\n",
    "---\n",
    "\n",
    "#### **9. Workflow Checklist**\n",
    "1. Understand the problem and define the task (regression/classification).\n",
    "2. Preprocess the data (handle missing values, scaling, encoding).\n",
    "3. Split the data (train/test split, cross-validation).\n",
    "4. Train the model and evaluate using appropriate metrics.\n",
    "5. Tune hyperparameters and refine the model.\n",
    "6. Test the model on unseen data.\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
